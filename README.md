# databricks-dbt-pipeline

# Databricks + dbt End-to-End Data Engineering Project

This project simulates a real-world, scalable data pipeline using the modern data stack — combining Databricks, Delta Lake, and dbt (Data Build Tool). It demonstrates a complete ETL flow from raw ingestion to business-ready reporting tables, following the medallion architecture (bronze → silver → gold).

##  Tech Stack
- Databricks – compute environment, notebook-based processing
- Apache Spark (PySpark & SQL) – data ingestion, transformation
- Delta Lake – storage format with ACID guarantees
- dbt – modular SQL modeling, testing, documentation
- Databricks Jobs – orchestration and job scheduling


## Key Highlights
- Built scalable, cloud-native pipeline from scratch
- Applied modular, testable transformations using dbt
- Demonstrated data versioning and lineage via Delta Lake + dbt docs
